{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import re\n",
    "import html\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from requests import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_dict = {\n",
    "        '1': \"I\",\n",
    "        '1A': \"I A\",\n",
    "        '1B': \"I B\",\n",
    "        '1C': \"I C\",\n",
    "        '2': \"II\",\n",
    "        '3': \"III\",\n",
    "        '4': \"IV\",\n",
    "        '5': \"V\",\n",
    "        '6': \"VI\",\n",
    "        '7': \"VII\",\n",
    "        '7A': \"VII A\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_indices(file_text, substring):\n",
    "    try:\n",
    "        item_no = substring.split()[1].strip('.')\n",
    "        pattern = r'(?i)(?:^|\\n)(I[\\s\\n]*t[\\s\\n]*e[\\s\\n]*m|I[\\s\\n]*t[\\s\\n]*e[\\s\\n]*m[\\s\\n]*s[\\s\\n]*)\\s*{}\\b\\s*[.:\\\\-]*'\n",
    "        pattern = pattern.format(item_no)\n",
    "        matches = [match.start() for match in re.finditer(pattern, file_text, re.MULTILINE)]\n",
    "        if matches==[]:\n",
    "            roman_no = roman_dict[item_no]\n",
    "            pattern = r'(?i)(?:^|\\n)(I[\\s\\n]*t[\\s\\n]*e[\\s\\n]*m|I[\\s\\n]*t[\\s\\n]*e[\\s\\n]*m[\\s\\n]*s[\\s\\n]*)\\s*{}\\b\\s*[.:\\\\-]*'\n",
    "            pattern = pattern.format(roman_no)\n",
    "            matches = [match.start() for match in re.finditer(pattern, file_text, re.MULTILINE)]\n",
    "    except:\n",
    "        matches = []\n",
    "    return matches\n",
    "\n",
    "def find_item_text(file_text, substring, index):\n",
    "    item_indices = find_all_indices(file_text, substring)\n",
    "    if substring=='ITEM 1.' and index!=0:\n",
    "        item_indices = [i for i in item_indices if i >= index]\n",
    "    elif substring!='ITEM 1.':\n",
    "        item_indices = [i for i in item_indices if i >= index]\n",
    "\n",
    "    if item_indices:\n",
    "        for start in item_indices:\n",
    "            item_start_index = start\n",
    "            startcheck = start\n",
    "            if file_text[item_start_index]=='\\n':\n",
    "                startcheck +=1\n",
    "            lines = file_text.splitlines()\n",
    "            matching_line = None\n",
    "            current_index = 0\n",
    "            for i, line in enumerate(lines):\n",
    "                line_length = len(line)\n",
    "                if i < len(lines) - 1:\n",
    "                    line_length += 1  \n",
    "                if current_index <= startcheck < current_index + line_length:\n",
    "                    matching_line = line\n",
    "                    break\n",
    "                current_index += line_length\n",
    "            line = matching_line\n",
    "            if line==None:\n",
    "                pass\n",
    "            elif '\"' not in line and '“' not in line and '”' not in line and '...' not in line and ',' not in line:\n",
    "                item_start_index = start\n",
    "                break\n",
    "        pattern = re.compile(r'(?i)(?:^|\\n)(I[\\s\\n]*t[\\s\\n]*e[\\s\\n]*m|I[\\s\\n]*t[\\s\\n]*e[\\s\\n]*m[\\s\\n]*s[\\s\\n]*)\\s*[0-9A-C]+\\s*[.:\\\\-]*', re.MULTILINE)\n",
    "        stop = False\n",
    "        count=0\n",
    "        start_index = item_start_index\n",
    "        while True:\n",
    "            count+=1\n",
    "            if stop == True:\n",
    "                break\n",
    "            if count>15:\n",
    "                return '', -1\n",
    "            match = pattern.search(file_text, pos=start_index+10)\n",
    "            if match:\n",
    "                stop_index = match.start()\n",
    "                item_stop_index = stop_index\n",
    "                stopcheck = stop_index\n",
    "                if file_text[item_stop_index]=='\\n':\n",
    "                    stopcheck+=1\n",
    "                matching_line = None\n",
    "                current_index = 0\n",
    "                for i, line in enumerate(lines):\n",
    "                    line_length = len(line)\n",
    "                    if i < len(lines) - 1:\n",
    "                        line_length += 1  \n",
    "                    if current_index <= stopcheck < current_index + line_length:\n",
    "                        matching_line = line\n",
    "                        break\n",
    "                    current_index += line_length\n",
    "                line = matching_line\n",
    "                if line==None:\n",
    "                    start_index = stop_index\n",
    "                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        if '\"' not in line and '“' not in line and '”' not in line and '...' not in line and ',' not in line:\n",
    "                            item_stop_index = stop_index\n",
    "                            item_text  = file_text[item_start_index:item_stop_index]\n",
    "                            stop = True\n",
    "                            return item_text, item_stop_index \n",
    "                        else:\n",
    "                            start_index = stop_index \n",
    "                            continue\n",
    "                    except:\n",
    "                        start_index = stop_index\n",
    "                        continue\n",
    "            else:\n",
    "                break\n",
    "        return '', -1\n",
    "    else:\n",
    "        return '', -1\n",
    "\n",
    "def clean_data(text):\n",
    "    #text = re.sub(r'<.*?>', '', text)\n",
    "    #text = re.sub(r'-\\d+-', '', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = {\n",
    "    'ITEM 1.':\"Item 1. Business\",\n",
    "    'ITEM 1A.': \"Item 1A. Risk Factors\",\n",
    "    'ITEM 1B.' : \"Item 1B. Unresolved Staff Comments\",\n",
    "    'ITEM 1C.' : \"Item 1C. Cybersecurity\",\n",
    "    'ITEM 2.': \"Item 2. Properties\",\n",
    "    'ITEM 3.' : \"Item 3. Legal Proceedings\",\n",
    "    'ITEM 4.' : \"Item 4. Mine Safety Disclosures\",\n",
    "    'ITEM 5.': \"Item 5. Market for Registrant’s Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    'ITEM 7.' : \"Item 7. Management’s Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    'ITEM 7A.' : \"Item 7A. Quantitative and Qualitative Disclosures About Market Risk\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will generate a csv file of all the text present in the URL. Generated CSV is used for copying and pasting the required items.\n",
    "Required items are :\n",
    "Item 1. Business,\t\n",
    "Item 1A. Risk Factors,\t\n",
    "Item 1B. Unresolved Staff Comments, \n",
    "Item 1C. Cybersecurity,\t\n",
    "Item 2. Properties,\t\n",
    "Item 3. Legal Proceedings, \n",
    "Item 4. Mine Safety Disclosures,\t\n",
    "Item 5. Market for Registrant’s Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\t\n",
    "Item 7. Management’s Discussion and Analysis of Financial Condition and Results of Operations,\t\n",
    "Item 7A. Quantitative and Qualitative Disclosures About Market Risk\n",
    "'''\n",
    "def extract_info_from_url(url):\n",
    "    try:\n",
    "        max_retries = 3\n",
    "        retry_delay = 3 \n",
    "        for _ in range(max_retries):\n",
    "            try:\n",
    "                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "            except HTTPError as e:\n",
    "                if e.response.status_code == 503:\n",
    "                    print(f\"Failed to retrieve content. Status code: {e.response.status_code}\")\n",
    "                    time.sleep(retry_delay)  \n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"HTTP error: {e}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break\n",
    "        #headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        #response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            text = response.text \n",
    "            #soup = BeautifulSoup(text, 'html.parser')\n",
    "            #text = soup.get_text()\n",
    "            text = html.unescape(text)\n",
    "            text=text.replace('&nbsp;', '')\n",
    "            text=text.replace('&#151;', '-')\n",
    "            text=text.replace('&#160;', '-')\n",
    "            text=text.replace('\\xa0', ' ')\n",
    "            part_url=['https://www.sec.gov/Archives/edgar/data/23197/0001169232-02-002122.txt','https://www.sec.gov/Archives/edgar/data/70145/0000070145-03-000112.txt' ,'https://www.sec.gov/Archives/edgar/data/70793/0000910647-03-000404.txt','https://www.sec.gov/Archives/edgar/data/1014052/0000096313-03-000216.txt','https://www.sec.gov/Archives/edgar/data/38777/0000038777-03-000693.txt', 'https://www.sec.gov/Archives/edgar/data/1014052/0001015402-04-004698.txt','https://www.sec.gov/Archives/edgar/data/1046050/0000939057-04-000463.txt','https://www.sec.gov/Archives/edgar/data/1283858/0000939057-04-000485.txt','https://www.sec.gov/Archives/edgar/data/717724/0000717724-04-000008.txt','https://www.sec.gov/Archives/edgar/data/1046050/0000939057-05-000360.txt','https://www.sec.gov/Archives/edgar/data/809933/0000809933-05-000019.txt','https://www.sec.gov/Archives/edgar/data/1046050/0000939057-07-000444.txt','https://www.sec.gov/Archives/edgar/data/1305014/0001305014-07-000308.txt','https://www.sec.gov/Archives/edgar/data/1046050/0000939057-08-000478.txt','https://www.sec.gov/Archives/edgar/data/1305014/0001305014-08-000364.txt','https://www.sec.gov/Archives/edgar/data/1046050/0000939057-09-000370.txt','https://www.sec.gov/Archives/edgar/data/1305014/0001305014-09-000097.txt','https://www.sec.gov/Archives/edgar/data/1046050/0000939057-10-000384.txt','https://www.sec.gov/Archives/edgar/data/1305014/0001305014-10-000079.txt']\n",
    "\n",
    "            text = re.sub(r'<!--[\\s\\S]*?-->|<[^>]*>', '\\n', text)\n",
    "            emptypattern = re.compile(r'^\\s*$', re.MULTILINE)\n",
    "            text = re.sub(emptypattern, '', text)\n",
    "            lines = [line.lstrip() for line in text.split('\\n')]\n",
    "            text = '\\n'.join(lines)\n",
    "            flag = False\n",
    "            table_stop_index = 0\n",
    "            table_stop_pattern = re.compile(r'Signatures', re.IGNORECASE)\n",
    "            table_start_index = text.lower().find(\"table of contents\")\n",
    "            if table_start_index!=-1: \n",
    "                start_index = 0\n",
    "                while True:\n",
    "                    toc_index = text.lower().find(\"table of contents\", start_index)\n",
    "                    if toc_index==-1:\n",
    "                        toc_index = text.lower().find(\"index to annual report on form 10-k\", start_index)\n",
    "                    if flag==True:\n",
    "                        break\n",
    "                    if toc_index != -1:\n",
    "                        match = table_stop_pattern.search(text, pos=toc_index+1)\n",
    "                        if match:\n",
    "                            table_stop_index = match.start()\n",
    "                            content_after_toc = text[toc_index:table_stop_index]\n",
    "                            content_lines = content_after_toc.split('\\n')\n",
    "                            found_items = [re.search(r'(?i)Item\\s*[0-9A-C]+\\s*[.:\\\\-]*', line) for line in content_lines]\n",
    "                            if any(found_items):\n",
    "                                table_start_index = toc_index\n",
    "                                flag=True\n",
    "                                break \n",
    "                            else:\n",
    "                                start_index = toc_index + 10\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if len(text[table_start_index:table_stop_index])<=3500 and table_stop_index!=0:\n",
    "                    text = text[:table_start_index] + text[table_stop_index:]\n",
    "                   \n",
    "            table_pattern =r'\\s*Item\\s*[0-9A-C]+\\s*[.:\\\\-]*\\s+[A-Za-z0-9\\s\\'\"\\-,]+\\.*\\s*\\.{3,}\\s*\\d+$'\n",
    "            part_pattern = r'\\bPART [I1]\\b'\n",
    "            part_start = []\n",
    "            for match in re.finditer(part_pattern, text, re.IGNORECASE):\n",
    "                part_start.append(match.start())\n",
    "            if len(part_start)>=2:\n",
    "                if part_start[1]-part_start[0]<=2500 or url in part_url:\n",
    "                    if text[part_start[0]:part_start[1]].lower().count('item')>10:\n",
    "                        text = text[part_start[1]:]\n",
    "            text=text.replace('Item l.', 'Item 1.')\n",
    "            text=text.replace('ITEM l.', 'Item 1.')\n",
    "            ctext=clean_data(text)\n",
    "            correct_df = pd.DataFrame({'Cleaned_Text': [ctext]})\n",
    "            correct_df.to_csv(\"correct.csv\", index=False)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(url, e)\n",
    "   \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "#change the url only. URL for file having incorrect or no extraction.\n",
    "url = 'https://www.sec.gov/Archives/edgar/data/908254/0000908254-04-000018.txt'\n",
    "extract_info_from_url(url)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
